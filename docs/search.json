[
  {
    "objectID": "research.html#arxiv",
    "href": "research.html#arxiv",
    "title": "Research",
    "section": "Arxiv",
    "text": "Arxiv"
  },
  {
    "objectID": "research.html#deans-summer-research-fellowship",
    "href": "research.html#deans-summer-research-fellowship",
    "title": "Research",
    "section": "Deans’ Summer Research Fellowship",
    "text": "Deans’ Summer Research Fellowship\nSummer of 2024, I worked independently under Prof. Alexander Volfovsky & Prof. Robin Evans investigating noisy measurements of confounders.\nI presented a poster at the Undergraduate Research symposium, which can be found here.\nFurther, I created software for the new method, which can be found here."
  },
  {
    "objectID": "research.html#summer-institute-in-biomedical-informatics-sibmi",
    "href": "research.html#summer-institute-in-biomedical-informatics-sibmi",
    "title": "Research",
    "section": "Summer Institute in Biomedical Informatics (SIBMI)",
    "text": "Summer Institute in Biomedical Informatics (SIBMI)\n\nSummer 2023 I worked in CELEHS Lab with Prof. Tianxi Cai and Prof. Junwei Lu.\n\n\n\nResearch Presentation\n\n\n\nPresentation\n\n\n\n\nA video I made regarding Medical Domain Contrastive Learning can be seen below"
  },
  {
    "objectID": "research.html#inspire-lab",
    "href": "research.html#inspire-lab",
    "title": "Research",
    "section": "InSPIre Lab",
    "text": "InSPIre Lab\n Lab Website."
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Projects",
    "section": "",
    "text": "For team members, visit our Password Protected Team Website."
  },
  {
    "objectID": "projects.html#academic-projects",
    "href": "projects.html#academic-projects",
    "title": "Projects",
    "section": "Academic Projects",
    "text": "Academic Projects\n\nIncreasing Life Expectancy with Controllable Variables\n\n\n\nReport\n\n\n\n\n\nPredicting Turbulence and Posterior Distributions\n\n\n\nReport"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Chase Mathis",
    "section": "",
    "text": "I am a first year Statistics PhD student at UC Berkeley. Previously, I graduated from Duke University with a BS in Mathematics and Statistics (High Distinction). My interests include (but are not limited to): causal inference, statistical methodology, Bayesian statistics, and distribution free inference\nContact: cmathis [at] berkeley [dot] edu\nOffice : 357 Evans Hall, Berkeley CA, 94704"
  },
  {
    "objectID": "index.html#education",
    "href": "index.html#education",
    "title": "Chase Mathis",
    "section": "",
    "text": "UC Berkeley Statistics PhD Candidate\nDuke University BS Mathematics and Statistics (High Distinction)\nContact: cmathis [at] berkeley [dot] edu."
  },
  {
    "objectID": "research.html#honors-thesis-supported-by-deans-summer-research-fellowship",
    "href": "research.html#honors-thesis-supported-by-deans-summer-research-fellowship",
    "title": "Research",
    "section": "Honors Thesis (Supported by Deans’ Summer Research Fellowship)",
    "text": "Honors Thesis (Supported by Deans’ Summer Research Fellowship)\nSummer of 2024, I worked independently under Prof. Alexander Volfovsky & Prof. Robin Evans investigating noisy measurements of confounders."
  },
  {
    "objectID": "research.html",
    "href": "research.html",
    "title": "Research",
    "section": "",
    "text": "See my Google Scholar and Arxiv for a full list.\nMy undergraduate honors thesis (Supported by Deans’ Summer Research Fellowship) focused on controlling for noisy measurements in a Bayesian manner. My advisors were Prof. Alexander Volfovsky & Prof. Robin Evans, and my full committe included Prof. Peter Hoff & Prof. Surya Tokdar."
  },
  {
    "objectID": "research.html#articles",
    "href": "research.html#articles",
    "title": "Research",
    "section": "",
    "text": "See my Google Scholar and Arxiv for a full list.\nMy undergraduate honors thesis (Supported by Deans’ Summer Research Fellowship) focused on controlling for noisy measurements in a Bayesian manner. My advisors were Prof. Alexander Volfovsky & Prof. Robin Evans, and my full committe included Prof. Peter Hoff & Prof. Surya Tokdar."
  },
  {
    "objectID": "research.html#software",
    "href": "research.html#software",
    "title": "Research",
    "section": "Software",
    "text": "Software\nordinalconfounder\nsurvivl\ncausl"
  },
  {
    "objectID": "honorsthesis.html",
    "href": "honorsthesis.html",
    "title": "A New Method to Adjust for Ordinal Variables With a Pre-Diabetes Case Study",
    "section": "",
    "text": "This browser does not support PDFs. Please download the PDF to view it: &lt;a href=\"docs/PDF_Files/StatThesis.pdf\"&gt;Download PDF&lt;/a&gt;."
  },
  {
    "objectID": "honorsthesis.html#articles",
    "href": "honorsthesis.html#articles",
    "title": "A New Method to Adjust for Ordinal Variables With a Pre-Diabetes Case Study",
    "section": "",
    "text": "Advisors: Prof. Alexander Volfovsky & Prof. Robin Evans\nCommittee: Prof. Peter Hoff & Prof. Surya Tokdar"
  },
  {
    "objectID": "honorsthesis.html#software",
    "href": "honorsthesis.html#software",
    "title": "A New Method to Adjust for Ordinal Variables With a Pre-Diabetes Case Study",
    "section": "Software",
    "text": "Software\nordinalconfounder\nsurvivl\ncausl"
  },
  {
    "objectID": "honorsthesis.html#article",
    "href": "honorsthesis.html#article",
    "title": "A New Method to Adjust for Ordinal Variables With a Pre-Diabetes Case Study",
    "section": "",
    "text": "This browser does not support PDFs. Please download the PDF to view it: &lt;a href=\"docs/PDF_Files/StatThesis.pdf\"&gt;Download PDF&lt;/a&gt;."
  },
  {
    "objectID": "honorsthesis.html#text-2",
    "href": "honorsthesis.html#text-2",
    "title": "A New Method to Adjust for Ordinal Variables With a Pre-Diabetes Case Study",
    "section": "Text 2",
    "text": "Text 2"
  },
  {
    "objectID": "statistics-diary.html",
    "href": "statistics-diary.html",
    "title": "Statistics I’ve Thought About",
    "section": "",
    "text": "A collection of topics in Statitsics I’m thinking about.\n\nSpring 2026\nTaking N(ormal) to infinity"
  },
  {
    "objectID": "notes/types-of-sampling.html",
    "href": "notes/types-of-sampling.html",
    "title": "types-of-sampling",
    "section": "",
    "text": "header\nHello"
  },
  {
    "objectID": "notes/Asymptotics.html",
    "href": "notes/Asymptotics.html",
    "title": "What Happened to Normal Approximations?",
    "section": "",
    "text": "This semester I will be taking STAT 210B, which has the generic name “Theoretical Statistics”. I predict the course will go through empirical process theory and topics like VC-dimension, concentration inequalities, and the symmetrization lemma.\nMy instructor, Prof. Nikita Zhivotovskiy, remarked in the first day of class, that the class will have a lens more toward finite sample results rather than asymptotics. Previoulsy, the class had a few more lectures dedicated to asympotitc statistics. He claimed that in the current day using finite sample results is more intuitive. Most theorems in ML conference papers use finite sample results to prove their theorems. Generally, he claimed thinking in terms of asymptotics is an old school approach in statistics, so much so that one of his asymptotic papers couldn’t find reviewers.\nSo, what happened to asymptotics? Prof. Lucien LeCam, an early professor of Statistics at Berkeley, is often given the title: “father of modern asymptotics”. His fundamental result of Local Asymptotic Normality is not taught in the PhD curriculum. I’m curious if it ever was. But if I described to him the size of datasets today, would he be surprised that asymptotics has fallen by the wayside?\nA defense of concentration inequalities is that datasets did not only grow longer, they also grew wider. This means that for a dataset described by a matrix with n rows and p columns, n grew but so did p. And many modern results consider the “regime” where \\(\\frac{n}{p} \\to \\alpha \\in (0,\\infty)\\). The tools of LeCam were not designed to deal with diagonally growing datasets, only taller growing ones.\nI will concede that concentration inequalities are well suited for diagnolly growing datasets. However, even when datasets grow only taller and not wider, there is a culture that says a researcher should use concentration inequalities. For instance, research in Bandits assume bounded rewards and then apply Hoefdding’s ad nauseam. At the end, they say “Machine learning uses lots of data, lets take \\(n \\to \\infty\\)”.\nA result I came across did this. It used concentration inequalities then let \\(n \\to \\infty\\). I wondered could we first let \\(n \\to \\infty\\) appealing to normality, and then do the analysis. I did and the result was the same if not a bit stronger. Maybe I’ll circle back to this when I think if its of any importance.\nBut I think appealing to normality is nice and under appreciated. I have a belief that most things in nature with large enough datasets (especially now) are somewhat normal. I would recommend people ask “can I appeal to normality?” and “what happens if I do?”. Morally the results should be about the same, and if they’re not that’s interesting! There’s much more to this, but I think normal approximations have the same fate linear regression does. Because it is taught in STAT 101, nobody likes to use it. But in reality, it’s pretty good and for most cases it is the best."
  },
  {
    "objectID": "writing.html",
    "href": "writing.html",
    "title": "Writing",
    "section": "",
    "text": "A collection of topics in Statitsics I’m thinking about.\n\nSpring 2026\nTaking N(ormal) to infinity - January 25\n\n\nBook & Movie Reviews\nGoodreads\nLetterboxd"
  }
]